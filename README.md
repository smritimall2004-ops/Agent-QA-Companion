# Ingestion & Normalization Layer

## Overview

Production-grade Python implementation for normalizing heterogeneous defect inputs (error logs, bug reports, free text) into a canonical schema for AI-assisted test generation. The system extracts structured fields from unstructured inputs using deterministic pattern matching and optional NLP enrichment.

**Current Status**: 85% complete - Core extraction engine fully functional and tested at **0.74 average confidence** with **100% field extraction rate** on real-world logs.

## What This System Does (For New Team Members)

This component is the **first step in the AI test generation pipeline**. It takes messy, unstructured bug reports and transforms them into clean, structured data that AI agents can understand.

**Input** ‚Üí Anything describing a bug:
- Raw error logs from servers (`.txt`, `.log` files)
- Copy-pasted text from Slack/emails
- Azure DevOps work items
- PDF/DOCX bug reports *(coming soon)*

**Output** ‚Üí Structured JSON with 5 key fields:
1. **error_type**: What went wrong (e.g., "Connection pool exhausted", "NullPointerException")
2. **component_module**: Which part broke (e.g., "database-service", "payment-api")
3. **trigger_repro_steps**: How to reproduce it
4. **observed_behavior**: What actually happened
5. **expected_behavior**: What should have happened

Each field includes a **confidence score** (0.0-1.0) so downstream AI agents know which data to trust.

## Features Status

### ‚úÖ Fully Implemented & Tested
- ‚úÖ **Free text ingestion** - Handles copy-pasted bug descriptions (tested: 0.95 confidence)
- ‚úÖ **Error log ingestion** - Processes `.txt` and `.log` files (tested: 0.74 confidence, 100% extraction)
- ‚úÖ **Pattern matching engine** - 65+ regex patterns optimized for microservice logs
- ‚úÖ **Confidence scoring** - Field-level confidence with source weighting
- ‚úÖ **ADO work item parsing** - Parses Azure DevOps bug format
- ‚úÖ **Type safety** - Full Pydantic validation with type hints
- ‚úÖ **Security utilities** - Input validation and size limits (not yet integrated)
- ‚úÖ **Comprehensive testing** - Unit tests + real-world log validation

### ‚ö†Ô∏è Partially Implemented
- ‚ö†Ô∏è **PDF extraction** - Placeholder only (library installed, needs implementation)
- ‚ö†Ô∏è **DOCX extraction** - Placeholder only (library installed, needs implementation)
- ‚ö†Ô∏è **NLP enrichment** - Structure complete but API integration pending
- ‚ö†Ô∏è **ADO API integration** - Parser works but REST API calls not implemented
- ‚ö†Ô∏è **Security enforcement** - SecurityValidator exists but not called from handlers

## Quick Setup (5 Minutes)

### 1. Create Virtual Environment
```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/Mac
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

**Key Dependencies**:
- `pydantic==2.12.5` - Data validation and schema management
- `python-dotenv` - Environment variable management
- `PyPDF2` - PDF extraction (not yet implemented)
- `python-docx` - DOCX extraction (not yet implemented)
- `pytest` - Testing framework

### 3. Verify Installation
```bash
cd examples
python run_all_demos.py
```

Expected output: 3 successful normalizations with confidence scores ~0.70-0.95

## Quick Start Examples

### Example 1: Process Free Text (Copy-Pasted Bug Description)
```python
from orchestration.pipeline import IngestionNormalizationPipeline

# Initialize pipeline (NLP enrichment disabled by default)
pipeline = IngestionNormalizationPipeline()

# Process a bug description copy-pasted from Slack/email
bug = pipeline.process_freetext("""
[2024-12-23 14:32:15] ERROR [payment-service] Connection pool exhausted
Failed to acquire database connection after 30s timeout
Customer impact detected: 45 users affected
Error occurred when processing payment for order #12345
Expected: Connection acquired within 5s per SLA
""")

# Access extracted fields with confidence scores
print(f"Error Type: {bug.error_type.value} (confidence: {bug.error_type.confidence})")
print(f"Component: {bug.component_module.value}")
print(f"Overall Confidence: {bug.overall_confidence}")

# Export to JSON for downstream AI agents
json_output = bug.model_dump_json(indent=2)
```

**Output**:
```
Error Type: Connection pool exhausted (confidence: 0.95)
Component: payment-service
Overall Confidence: 0.87
```

### Example 2: Process Error Log File
```python
# Process a .log or .txt file
bug = pipeline.process_error_log("logs/production_error.log")

# Check which fields were successfully extracted
extracted_fields = [f for f in ['error_type', 'component_module', 
                                 'trigger_repro_steps', 'observed_behavior',
                                 'expected_behavior'] 
                    if getattr(bug, f).value is not None]
print(f"Extracted {len(extracted_fields)}/5 fields")
```

### Example 3: Process Azure DevOps Bug
```python
# Process ADO work item (expects dict format from ADO REST API)
ado_bug = {
    'id': 12345,
    'fields': {
        'System.Title': 'Payment API timeout',
        'System.Description': 'Users unable to complete checkout...',
        'Microsoft.VSTS.TCM.ReproSteps': '1. Add item to cart 2. Click checkout',
        'Microsoft.VSTS.Common.AcceptanceCriteria': 'Payment completes in <2s'
    }
}

bug = pipeline.process_ado_bug(ado_bug)
```

## System Architecture (How It Works)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         INPUT SOURCES                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Free Text  ‚îÇ  Error Logs (.txt/.log)  ‚îÇ  Azure DevOps  ‚îÇ  PDF/DOCX ‚îÇ
‚îÇ   (working) ‚îÇ       (working)          ‚îÇ   (working)    ‚îÇ  (pending) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                   ‚îÇ                      ‚îÇ
       v                   v                      v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INGESTION HANDLERS                                ‚îÇ
‚îÇ  - Validate file size (<50MB)                                       ‚îÇ
‚îÇ  - Extract raw text                                                 ‚îÇ
‚îÇ  - Sanitize input (remove control chars)                           ‚îÇ
‚îÇ  - Generate source metadata                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ Raw Text
                               v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  EXTRACTION ENGINE (Pattern Registry v2.0.0)         ‚îÇ
‚îÇ  - Try STRICT patterns first (confidence: 0.90-1.00)                ‚îÇ
‚îÇ  - Fallback to LOOSE patterns (confidence: 0.40-0.70)               ‚îÇ
‚îÇ  - Extract 5 core fields with evidence strings                      ‚îÇ
‚îÇ  - 65+ regex patterns for microservice logs                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ Extraction Results
                               v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CONFIDENCE SCORER                                ‚îÇ
‚îÇ  - Apply source weights (ADO_API: 1.0, REGEX_STRICT: 0.95)         ‚îÇ
‚îÇ  - Evidence boost (+0.05 for non-empty evidence)                   ‚îÇ
‚îÇ  - Short value penalty (0.8x for values <5 chars)                  ‚îÇ
‚îÇ  - Calculate overall confidence (avg √ó completeness)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ Confidence Scores
                               v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              OPTIONAL NLP ENRICHER (Currently Disabled)              ‚îÇ
‚îÇ  - Enrich fields with confidence < 0.5                              ‚îÇ
‚îÇ  - Call LLM API to infer missing fields                            ‚îÇ
‚îÇ  - Never overwrites high-confidence extractions                     ‚îÇ
‚îÇ  - Status: Structure complete, API integration pending              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ Enriched Fields
                               v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NORMALIZED BUG SCHEMA                             ‚îÇ
‚îÇ  {                                                                  ‚îÇ
‚îÇ    "bug_id": "uuid",                                               ‚îÇ
‚îÇ    "error_type": {"value": "...", "confidence": 0.95},             ‚îÇ
‚îÇ    "component_module": {...},                                      ‚îÇ
‚îÇ    "trigger_repro_steps": {...},                                   ‚îÇ
‚îÇ    "observed_behavior": {...},                                     ‚îÇ
‚îÇ    "expected_behavior": {...},                                     ‚îÇ
‚îÇ    "overall_confidence": 0.87,                                     ‚îÇ
‚îÇ    "source_metadata": {...}                                        ‚îÇ
‚îÇ  }                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ JSON Output
                               v
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  AI TEST GENERATOR   ‚îÇ
                    ‚îÇ    (Downstream)      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components Explained

**1. Ingestion Handlers** (`ingestion/`)
- **FreeTextHandler**: Processes copy-pasted text (Slack, email, tickets)
- **ErrorLogHandler**: Extracts text from log files (.txt, .log, .pdf*, .docx*)
- **AzureDevOpsHandler**: Parses ADO work item JSON format
- *Note: PDF/DOCX extraction pending implementation*

**2. Pattern Registry** (`extraction/pattern_registry.py`)
- Version: **2.0.0** (completely rewritten for microservice logs)
- Contains **65+ regex patterns** across 5 categories
- Patterns optimized for format: `[timestamp] LEVEL [service-name] message`
- Examples:
  - Error types: "Connection pool exhausted", "Out of memory", "504 Gateway Timeout"
  - Components: `[database-service]`, `service: payment-api`
  - Observed behaviors: "Customer impact detected: X users"

**3. Rule Engine** (`extraction/rule_engine.py`)
- Applies patterns in order: STRICT ‚Üí LOOSE
- Returns best match with highest confidence
- Captures evidence string (max 500 chars)
- Occurrence boost: +0.1 confidence per additional match (max +0.2)

**4. Confidence Scorer** (`confidence/scorer.py`)
- Source weights determine baseline confidence
- Evidence presence adds +0.05
- Short values (<5 chars) penalized to 0.8x
- Overall = average field confidence √ó completeness factor

**5. Pipeline Orchestrator** (`orchestration/pipeline.py`)
- Coordinates all components
- Routes input type to correct handler
- Manages configuration and logging
- Returns validated `NormalizedBug` object

## Configuration Options

### Method 1: Environment Variables (`.env` file)

```bash
# Optional NLP enrichment (currently disabled)
ENABLE_NLP=false
LLM_API_ENDPOINT=https://api.openai.com/v1/chat/completions
LLM_API_KEY=your_api_key_here

# Azure DevOps integration
ADO_API_URL=https://dev.azure.com/yourorg/yourproject
ADO_API_TOKEN=your_personal_access_token

# Logging
LOG_LEVEL=INFO
```

### Method 2: Programmatic Configuration

```python
from config import PipelineConfig
from orchestration.pipeline import IngestionNormalizationPipeline

# Create custom configuration
config = PipelineConfig(
    enable_nlp_enrichment=False,          # NLP enrichment toggle
    confidence_threshold_high=0.8,        # High confidence cutoff
    confidence_threshold_medium=0.5,      # Medium confidence cutoff  
    max_file_size_mb=50,                  # File size limit
    redact_pii=True,                      # PII redaction in logs
    log_level="INFO"                      # Logging verbosity
)

# Initialize pipeline with config
pipeline = IngestionNormalizationPipeline(config=config)
```

### Configuration Parameters Explained

| Parameter | Default | Description |
|-----------|---------|-------------|
| `enable_nlp_enrichment` | `False` | Enable LLM-based field enrichment (requires API) |
| `confidence_threshold_high` | `0.8` | Minimum confidence for "high quality" label |
| `confidence_threshold_medium` | `0.5` | Minimum confidence for "acceptable" label |
| `max_file_size_mb` | `50` | Maximum file size for error logs |
| `redact_pii` | `True` | Redact PII in logging output |
| `log_level` | `"INFO"` | Logging verbosity (DEBUG/INFO/WARNING/ERROR) |

## Normalized Schema (Output Format)

Every input normalizes to this canonical structure:

```json
{
  "bug_id": "a3f7c2e1-4b5d-6789-0abc-def123456789",
  "schema_version": "1.0.0",
  "timestamp": "2024-12-23T14:32:15Z",
  
  "error_type": {
    "value": "Connection pool exhausted",
    "confidence": 0.95,
    "source": "regex_strict",
    "raw_evidence": "[2024-12-23 14:32:15] ERROR Connection pool exhausted..."
  },
  
  "component_module": {
    "value": "payment-service",
    "confidence": 0.95,
    "source": "regex_strict",
    "raw_evidence": "[payment-service] Connection pool exhausted"
  },
  
  "trigger_repro_steps": {
    "value": "Processing payment for order #12345 during high load",
    "confidence": 0.70,
    "source": "regex_loose",
    "raw_evidence": "Error occurred when processing payment for order #12345"
  },
  
  "observed_behavior": {
    "value": "Failed to acquire database connection after 30s timeout. Customer impact detected: 45 users affected",
    "confidence": 0.75,
    "source": "regex_loose",
    "raw_evidence": "Failed to acquire database connection after 30s timeout..."
  },
  
  "expected_behavior": {
    "value": "Connection acquired within 5s per SLA",
    "confidence": 0.90,
    "source": "regex_strict",
    "raw_evidence": "Expected: Connection acquired within 5s per SLA"
  },
  
  "overall_confidence": 0.85,
  "fields_extracted_by_regex": 5,
  "fields_enriched_by_nlp": 0,
  
  "source_metadata": {
    "source_type": "error_log",
    "source_id": "production_error_20241223.log",
    "ingestion_timestamp": "2024-12-23T14:35:00Z",
    "original_size_bytes": 2048,
    "file_hash": "sha256:abc123..."
  }
}
```

### Field Descriptions

| Field | Description | Example Values |
|-------|-------------|----------------|
| `error_type` | Type of error/defect | "NullPointerException", "504 Gateway Timeout", "Connection pool exhausted" |
| `component_module` | Which system/service failed | "database-service", "payment-api", "AuthenticationManager" |
| `trigger_repro_steps` | How to reproduce | "1. Login as admin 2. Click Export button" |
| `observed_behavior` | What actually happened | "System crashed with OOM error. 45 users impacted" |
| `expected_behavior` | What should happen | "Export completes in <5s without errors" |

### Confidence Score Interpretation

| Range | Label | Meaning | Action |
|-------|-------|---------|--------|
| **0.80 - 1.00** | HIGH | High-quality extraction, trust this data | Use as-is |
| **0.50 - 0.79** | MEDIUM | Decent extraction, possibly incomplete | Optional human review |
| **0.00 - 0.49** | LOW | Poor extraction, likely incorrect | Recommend NLP enrichment |

**Overall Confidence Formula**: `average_field_confidence √ó completeness_factor`
- Completeness factor = (fields_extracted / 5.0)
- Example: 4 fields with avg 0.85 confidence ‚Üí 0.85 √ó 0.8 = **0.68 overall**

## Real-World Performance (Tested)

### Test Setup
- **Test Date**: December 2024
- **Test Data**: 4 production-like microservice error logs
- **Log Format**: `[timestamp] LEVEL [service-name] message`
- **Test Script**: `test_real_logs.py`

### Results

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Average Confidence** | **0.74** | Medium-High quality |
| **Field Extraction Rate** | **100%** (20/20 fields) | All fields extracted |
| **High Confidence Fields** | 65% | Most fields trustworthy |
| **Medium Confidence Fields** | 35% | Acceptable quality |
| **Low Confidence Fields** | 0% | No poor extractions |

### Sample Extraction (database_connection_failure.log)

**Input** (excerpt):
```
[2024-12-20 09:15:33] ERROR [database-service] Connection pool exhausted
Failed to acquire database connection after 30s timeout
Customer impact detected: 45 users affected
```

**Extracted Fields**:
- ‚úÖ error_type: "Connection pool exhausted" (confidence: 0.95)
- ‚úÖ component_module: "database-service" (confidence: 0.95)
- ‚úÖ observed_behavior: "Customer impact detected: 45 users affected" (confidence: 0.75)
- ‚úÖ expected_behavior: "Connection acquired within 5s per SLA" (confidence: 0.90)
- ‚úÖ trigger_repro_steps: Extracted (confidence: 0.65)

### Pattern Registry Evolution

**v1.0.0** (Initial):
- Generic patterns like `(?i)error` and `class\s+(\w+)`
- **Results**: 40% extraction rate, 0.29 avg confidence ‚ùå

**v2.0.0** (Current):
- 65+ microservice-optimized patterns
- **Results**: 100% extraction rate, 0.74 avg confidence ‚úÖ
- **Improvement**: +155% confidence, +150% extraction rate

## Testing & Validation

### Run Automated Tests
```bash
# All unit tests
pytest tests/ -v

# Specific test module
pytest tests/test_extraction.py -v

# With coverage report
pytest --cov=. --cov-report=html tests/
```

### Run Demo Examples
```bash
# Single demo (free text processing)
cd examples
python demo.py

# Batch processing demo
python batch_demo.py

# All demos at once
python run_all_demos.py
```

### Test with Real Error Logs
```bash
# Place your .log files in test_data/
# Run comprehensive test
python test_real_logs.py

# Expected output:
# - Field-by-field extraction results
# - Confidence scores and distributions  
# - JSON outputs in test_data/
# - Statistics summary
```

### Test Data Location
- **Unit test data**: `tests/test_data/`
- **Real-world test logs**: `test_data/*.log`
- **Sample outputs**: `docs/sample_output.json`

## Extensibility & Customization

### Adding New Patterns

If extraction quality is poor for your log format, add custom patterns:

```python
# Edit: extraction/pattern_registry.py

class PatternRegistry:
    # Add to existing pattern lists
    ERROR_TYPE_STRICT = [
        # ... existing patterns ...
        (re.compile(r'YOUR_CUSTOM_PATTERN', re.IGNORECASE), 0.90),
    ]
```

**Pattern Guidelines**:
- **STRICT patterns**: High confidence (0.85-1.00), specific regex
- **LOOSE patterns**: Lower confidence (0.40-0.70), broader regex
- Include named capture groups: `(?P<name>...)`
- Test with real logs before committing

### Custom Ingestion Handler

Add support for new file formats:

```python
# Create: ingestion/custom_handler.py

from ingestion.base import BaseIngestionHandler
from schemas.source_metadata import SourceMetadata

class CustomFormatHandler(BaseIngestionHandler):
    def ingest(self, input_data) -> tuple[str, SourceMetadata]:
        # Your parsing logic
        raw_text = self._parse_custom_format(input_data)
        
        metadata = SourceMetadata(
            source_type="custom_format",
            source_id=str(input_data),
            ingestion_timestamp=datetime.now(timezone.utc),
            original_size_bytes=len(raw_text)
        )
        
        return raw_text, metadata
```

### Custom NLP Enrichment

Replace placeholder enrichment with real LLM integration:

```python
# Edit: enrichment/nlp_enricher.py

def _enrich_field(self, field_name: str, raw_text: str, 
                  bug: NormalizedBug) -> Optional[str]:
    """Call LLM API to enrich field"""
    
    prompt = f"Extract {field_name} from: {raw_text[:1000]}"
    
    # Call your LLM API (OpenAI, Anthropic, Azure OpenAI, etc.)
    response = call_llm_api(self.llm_endpoint, prompt)
    
    return response.get('enriched_value')
```

## Project Status & Remaining Work

### ‚úÖ Completed (85%)

**Core Extraction Engine** (100% functional)
- ‚úÖ Pattern Registry v2.0.0 with 65+ patterns
- ‚úÖ Rule-based extraction with strict/loose fallback
- ‚úÖ Confidence scoring with source weights
- ‚úÖ Pipeline orchestration
- ‚úÖ Pydantic schema validation

**Ingestion Handlers** (75% complete)
- ‚úÖ Free text ingestion (tested: 0.95 confidence)
- ‚úÖ Error log ingestion for `.txt` and `.log` (tested: 0.74 confidence)
- ‚úÖ ADO work item parser (dict format)
- ‚ö†Ô∏è PDF extraction (placeholder only) 
- ‚ö†Ô∏è DOCX extraction (placeholder only)

**Testing & Documentation** (100% complete)
- ‚úÖ Unit tests with pytest
- ‚úÖ Real-world log validation (4 test files)
- ‚úÖ Demo examples and batch processing
- ‚úÖ Comprehensive README

### ‚ö†Ô∏è Pending Work (15%)

| Task | Status | Priority | Estimated Effort |
|------|--------|----------|------------------|
| **Security validator integration** | Structure exists, not called | üî¥ Critical | 1-2 hours |
| **PDF extraction implementation** | Placeholder only | üü° Medium | 2-4 hours |
| **DOCX extraction implementation** | Placeholder only | üü° Medium | 2-4 hours |
| **NLP enrichment API integration** | Structure complete, no API | üî¥ High | 8-16 hours |
| **Azure DevOps REST API calls** | Parser works, no API client | üü° Medium | 4-8 hours |

### Pending Task Details

#### 1. Security Validator Integration (Critical)
**File**: `ingestion/base.py`, `ingestion/freetext_handler.py`, `ingestion/error_log_handler.py`

**Current State**: `SecurityValidator` class exists in `utils/security.py` but is not called from any ingestion handlers.

**Required Changes**:
```python
from utils.security import SecurityValidator

# In ErrorLogHandler.ingest()
if not SecurityValidator.validate_file_path(str(file_path)):
    raise ValueError(f"Security validation failed for: {file_path}")

# In FreeTextHandler.ingest()  
sanitized = SecurityValidator.sanitize_text(text, max_length=10000)
```

**Why Critical**: Security requirements not enforced (path traversal, dangerous extensions).

---

#### 2. PDF Extraction (Medium Priority)
**File**: `ingestion/error_log_handler.py:54-62`

**Current Code**:
```python
def _extract_from_pdf(self, file_path: Path) -> str:
    logger.warning(f"PDF extraction not yet implemented: {file_path}")
    return ""
```

**Required Implementation**:
```python
def _extract_from_pdf(self, file_path: Path) -> str:
    import PyPDF2
    
    with open(file_path, 'rb') as f:
        reader = PyPDF2.PdfReader(f)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
    
    return text
```

**Why Needed**: PDF logs common in production environments.

---

#### 3. DOCX Extraction (Medium Priority)
**File**: `ingestion/error_log_handler.py:64-72`

**Current Code**:
```python
def _extract_from_doc(self, file_path: Path) -> str:
    logger.warning(f"DOCX extraction not yet implemented: {file_path}")
    return ""
```

**Required Implementation**:
```python
def _extract_from_doc(self, file_path: Path) -> str:
    from docx import Document
    
    doc = Document(file_path)
    text = "\n".join([para.text for para in doc.paragraphs])
    
    return text
```

**Why Needed**: Manual bug reports often saved as DOCX.

---

#### 4. NLP Enrichment API Integration (High Priority)
**File**: `enrichment/nlp_enricher.py:82-91`

**Current Code**:
```python
def _enrich_field(self, field_name: str, raw_text: str, 
                  bug: NormalizedBug) -> Optional[str]:
    logger.debug(f"Would enrich {field_name} via LLM API")
    return None  # Placeholder
```

**Required Implementation**:
```python
def _enrich_field(self, field_name: str, raw_text: str, 
                  bug: NormalizedBug) -> Optional[str]:
    import openai
    
    prompt = self._build_prompt(field_name, raw_text, bug)
    
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    
    return response.choices[0].message.content.strip()
```

**Why High Priority**: Core feature for handling low-confidence extractions (<0.5).

---

#### 5. Azure DevOps REST API Client (Medium Priority)
**File**: `ingestion/ado_handler.py`

**Current State**: Handler parses ADO work item dict format but doesn't fetch from API.

**Required Addition**:
```python
import requests
from typing import List

class AzureDevOpsHandler(BaseIngestionHandler):
    # ... existing code ...
    
    def fetch_work_item(self, work_item_id: int) -> dict:
        """Fetch work item from ADO REST API"""
        url = f"{self.ado_url}/_apis/wit/workitems/{work_item_id}"
        headers = {"Authorization": f"Basic {self.api_token}"}
        
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        
        return response.json()
    
    def fetch_work_items_batch(self, ids: List[int]) -> List[dict]:
        """Batch fetch multiple work items"""
        # Implementation here
        pass
```

**Why Needed**: Enable direct ADO integration without manual export.

## Production Deployment Checklist

### Phase 1: Core Setup ‚úÖ (Complete)
- [x] Virtual environment configured
- [x] Dependencies installed (`requirements.txt`)
- [x] Unit tests passing
- [x] Demo examples validated
- [x] Real-world logs tested (0.74 avg confidence)

### Phase 2: Security & Validation ‚ö†Ô∏è (1-2 hours remaining)
- [ ] Integrate `SecurityValidator.validate_file_path()` in all handlers
- [ ] Integrate `SecurityValidator.sanitize_text()` in free text handler
- [ ] Test with malicious file paths (path traversal attempts)
- [ ] Test with dangerous file extensions (.exe, .bat, .ps1)
- [ ] Enable PII redaction in production logs

### Phase 3: File Format Support ‚ö†Ô∏è (4-8 hours remaining)
- [x] TXT/LOG extraction working
- [ ] Implement PDF extraction using PyPDF2
- [ ] Implement DOCX extraction using python-docx
- [ ] Test with production PDF/DOCX samples
- [ ] Handle corrupted file errors gracefully

### Phase 4: NLP Enrichment ‚ö†Ô∏è (8-16 hours remaining)
- [ ] Choose LLM provider (OpenAI, Azure OpenAI, Anthropic, etc.)
- [ ] Implement API client in `_enrich_field()`
- [ ] Design field-specific prompts
- [ ] Add retry logic and rate limiting
- [ ] Calibrate confidence scores for NLP-enriched fields
- [ ] Test with low-confidence extractions (<0.5)
- [ ] Measure accuracy improvement (before/after NLP)

### Phase 5: Azure DevOps Integration ‚ö†Ô∏è (4-8 hours remaining)
- [x] ADO work item parser implemented
- [ ] Implement REST API client (`fetch_work_item()`)
- [ ] Add batch fetch capability
- [ ] Handle authentication with PAT token
- [ ] Test with real ADO environment
- [ ] Add error handling for API failures

### Phase 6: Monitoring & Observability üìä (Recommended)
- [ ] Configure structured logging (JSON format)
- [ ] Set up metrics collection:
  - `fields_extracted_by_regex` - Extraction effectiveness
  - `fields_enriched_by_nlp` - NLP usage rate  
  - `overall_confidence_distribution` - Quality metrics
  - `processing_time_ms` - Performance tracking
- [ ] Set up alerting for:
  - Low confidence rate (>30% of bugs <0.5 confidence)
  - High processing time (>5s per bug)
  - API failures (NLP/ADO)
- [ ] Create operational dashboard

### Phase 7: Environment Configuration
```bash
# Required environment variables
ENABLE_NLP=true
LLM_API_ENDPOINT=https://api.openai.com/v1/chat/completions
LLM_API_KEY=sk-...
ADO_API_URL=https://dev.azure.com/yourorg/yourproject
ADO_API_TOKEN=your_pat_token
LOG_LEVEL=INFO
```

### Phase 8: Final Validation
- [ ] Test end-to-end with production data samples
- [ ] Validate output JSON schema with downstream AI agents
- [ ] Performance testing (target: <2s per bug)
- [ ] Load testing (target: 100 bugs/minute)
- [ ] Document custom patterns for your log formats
- [ ] Train team on adding new patterns
- [ ] Create runbook for common issues

---

## Monitoring Metrics (Production)

Key metrics to track in production:

| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| `overall_confidence` (avg) | >0.70 | <0.50 |
| `fields_extracted_by_regex` (avg) | >4.0/5 | <3.0/5 |
| `low_confidence_rate` (% <0.5) | <20% | >40% |
| `processing_time_ms` (p95) | <2000ms | >5000ms |
| `nlp_enrichment_success_rate` | >90% | <70% |
| `api_error_rate` (ADO/LLM) | <1% | >5% |

---

## Troubleshooting Common Issues

### Issue 1: Low Confidence Scores (<0.5)
**Symptoms**: Most extractions have confidence <0.5

**Diagnosis**:
```bash
# Check pattern matching effectiveness
python test_real_logs.py
# Review which fields are low confidence
```

**Solution**:
1. Review your log format - does it match microservice format `[timestamp] LEVEL [service-name] message`?
2. Add custom patterns in `extraction/pattern_registry.py`
3. Test new patterns before committing
4. Enable NLP enrichment for low-confidence fields

---

### Issue 2: No Fields Extracted (0/5)
**Symptoms**: All fields return `None`

**Diagnosis**:
```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

pipeline = IngestionNormalizationPipeline()
bug = pipeline.process_freetext("your text")
# Check logs for pattern matching attempts
```

**Solution**:
1. Your log format may be completely different
2. Add format-specific patterns
3. Consider using NLP enrichment as primary extraction

---

### Issue 3: PDF/DOCX Not Working
**Symptoms**: `logger.warning("PDF extraction not yet implemented")`

**Solution**: Implement extraction methods following instructions in "Pending Work" section above.

---

### Issue 4: NLP Enrichment Not Working
**Symptoms**: `fields_enriched_by_nlp` always 0

**Diagnosis**:
```python
config = PipelineConfig(enable_nlp_enrichment=True)
# Check if LLM_API_ENDPOINT is set
```

**Solution**:
1. Ensure `enable_nlp_enrichment=True`
2. Set `LLM_API_ENDPOINT` and `LLM_API_KEY`
3. Implement `_enrich_field()` method (currently placeholder)

---

## Directory Structure

```
ingestion_normalization/
‚îú‚îÄ‚îÄ config.py                    # Pipeline configuration
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ README.md                    # This file
‚îÇ
‚îú‚îÄ‚îÄ ingestion/                   # Input handlers
‚îÇ   ‚îú‚îÄ‚îÄ base.py                  # Abstract base handler
‚îÇ   ‚îú‚îÄ‚îÄ freetext_handler.py      # Free text ingestion ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ error_log_handler.py     # Log file ingestion (TXT/LOG ‚úÖ, PDF ‚ö†Ô∏è, DOCX ‚ö†Ô∏è)
‚îÇ   ‚îî‚îÄ‚îÄ ado_handler.py           # Azure DevOps parser ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ extraction/                  # Pattern matching engine
‚îÇ   ‚îú‚îÄ‚îÄ pattern_registry.py      # 65+ regex patterns v2.0.0 ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ rule_engine.py           # Extraction orchestration ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ extractors.py            # Field extractors ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ confidence/                  # Confidence scoring
‚îÇ   ‚îî‚îÄ‚îÄ scorer.py                # Deterministic scoring ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ enrichment/                  # Optional NLP enrichment
‚îÇ   ‚îú‚îÄ‚îÄ base.py                  # Abstract enricher
‚îÇ   ‚îî‚îÄ‚îÄ nlp_enricher.py          # LLM-based enrichment ‚ö†Ô∏è
‚îÇ
‚îú‚îÄ‚îÄ schemas/                     # Data schemas
‚îÇ   ‚îú‚îÄ‚îÄ normalized_bug.py        # Output schema ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ source_metadata.py       # Input metadata ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ orchestration/               # Pipeline coordination
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py              # Main orchestrator ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ utils/                       # Utilities
‚îÇ   ‚îú‚îÄ‚îÄ logging.py               # Structured logging ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ security.py              # Security validation ‚ö†Ô∏è
‚îÇ
‚îú‚îÄ‚îÄ tests/                       # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_extraction.py       # Extraction tests ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ test_components.py       # Component tests ‚úÖ
‚îÇ
‚îú‚îÄ‚îÄ test_data/                   # Real-world test logs
‚îÇ   ‚îú‚îÄ‚îÄ database_connection_failure_normalized.json
‚îÇ   ‚îú‚îÄ‚îÄ api_gateway_timeout_normalized.json
‚îÇ   ‚îú‚îÄ‚îÄ memory_leak_detection_normalized.json
‚îÇ   ‚îî‚îÄ‚îÄ microservice_cascade_failure_normalized.json
‚îÇ
‚îú‚îÄ‚îÄ examples/                    # Demo scripts
‚îÇ   ‚îú‚îÄ‚îÄ demo.py                  # Single example ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ batch_demo.py            # Batch processing ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ run_all_demos.py         # All demos ‚úÖ
‚îÇ
‚îî‚îÄ‚îÄ docs/                        # Documentation
    ‚îî‚îÄ‚îÄ sample_output.json       # Example output

Legend: ‚úÖ Complete | ‚ö†Ô∏è Pending | üî¥ Critical | üü° Medium Priority
```

---

## FAQ for New Team Members

### Q1: Where do I start?
**A**: Run the setup and demos:
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
pip install -r requirements.txt
cd examples
python run_all_demos.py
```

### Q2: How do I test with my own logs?
**A**: Place your `.log` files in `test_data/` and run:
```bash
python test_real_logs.py
```

### Q3: My extractions have low confidence. What do I do?
**A**: 
1. Check your log format - does it match `[timestamp] LEVEL [service-name] message`?
2. Add custom patterns in `extraction/pattern_registry.py`
3. Enable NLP enrichment (requires implementation)

### Q4: How do I add support for my log format?
**A**: Edit `extraction/pattern_registry.py` and add format-specific patterns. See "Extensibility" section.

### Q5: What's the difference between STRICT and LOOSE patterns?
**A**: 
- **STRICT** (0.85-1.00 confidence): Highly specific, rarely wrong
- **LOOSE** (0.40-0.70 confidence): Broader patterns, higher recall but more false positives

### Q6: Why is NLP enrichment disabled by default?
**A**: It requires LLM API integration (OpenAI/Anthropic) which:
- Costs money per API call
- Adds latency (~500ms-2s)
- Needs careful prompt engineering
- Should only enrich low-confidence fields (<0.5)

### Q7: Can I use this without Azure DevOps?
**A**: Yes! Free text and error log ingestion work independently. ADO is optional.

### Q8: What's the expected processing time?
**A**: 
- Free text: <100ms
- Error logs (.txt/.log): <500ms
- With NLP enrichment: 1-3 seconds (depends on LLM API)

---

## Support & Contact

- **Project Owner**: AI Systems Team
- **Repository**: [Internal Git URL]
- **Slack Channel**: #dora-ingestion-normalization
- **Documentation**: This README + inline code comments

For bugs or feature requests, create an issue in the project repository.
"""
